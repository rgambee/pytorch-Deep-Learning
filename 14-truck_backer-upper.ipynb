{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import *\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.lines import Line2D\n",
    "π = pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use(['dark_background', 'bmh'])\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car-trailer diagram (inverted image `res/car-trainer-k.png` available as well):\n",
    "![car-trailer](res/car-trailer-w.png)\n",
    "\n",
    "Car-trailer equation:\n",
    "\\begin{align}\n",
    "\\dot x &= s \\cos \\theta_0 \\\\\n",
    "\\dot y &= s \\sin \\theta_0 \\\\\n",
    "\\dot \\theta_0 &= \\frac{s}{L} \\tan \\phi \\\\\n",
    "\\dot \\theta_1 &= \\frac{s}{d_1} \\sin(\\theta_1 - \\theta_0)\n",
    "\\end{align}\n",
    "where $s$: signed speed, $\\phi$: negative steering angle,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT_BBOX = [0, 40, -10, 10]\n",
    "STEERING_ANGLE_RANGE_rad = pi / 4\n",
    "\n",
    "class Truck:\n",
    "    def __init__(self, display=False):\n",
    "\n",
    "        self.W = 1  # car and trailer width, for drawing only\n",
    "        self.L = 1 * self.W  # car length\n",
    "        self.d = 4 * self.L  # d_1\n",
    "        self.s = -0.1  # speed\n",
    "        self.display = display\n",
    "        \n",
    "        self.box = ENVIRONMENT_BBOX\n",
    "        if self.display:\n",
    "            self.f = figure(figsize=(10, 5), num='The truck backer-upper', facecolor='none')\n",
    "            self.ax = self.f.add_axes([0.01, 0.01, 0.98, 0.98], facecolor='black')\n",
    "            self.patches = list()\n",
    "            \n",
    "            self.ax.axis('equal')\n",
    "            b = self.box\n",
    "            self.ax.axis([b[0] - 1, b[1], b[2], b[3]])\n",
    "            self.ax.set_xticks([]); self.ax.set_yticks([])\n",
    "            self.ax.axhline(); self.ax.axvline()\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, ϕ=0):\n",
    "        self.ϕ = ϕ  # car initial steering angle\n",
    "        \n",
    "        # self.θ0 = deg2rad(30)  # car initial direction\n",
    "        # self.θ1 = deg2rad(-30)  # trailer initial direction\n",
    "        # self.x, self.y = 20, -5  # initial car coordinates\n",
    "        \n",
    "        self.θ0 = random() * 2 * π  # 0 <= ϑ₀ < 2π\n",
    "        self.θ1 = (random() - 0.5) * π / 2 + self.θ0  # -π/4 <= ϑ₁ - ϑ₀ < π/4\n",
    "        self.x = (random() * .75 + 0.25) * self.box[1]\n",
    "        self.y = (random() - 0.5) * (self.box[3] - self.box[2])\n",
    "        \n",
    "        # If poorly initialise, then re-initialise\n",
    "        if not self.valid():\n",
    "            self.reset(ϕ)\n",
    "        \n",
    "        # Draw, if display is True\n",
    "        if self.display: self.draw()\n",
    "    \n",
    "    def step(self, ϕ=0, dt=1):\n",
    "        \n",
    "        # Check for illegal conditions\n",
    "        if self.is_jackknifed():\n",
    "            print('The truck is jackknifed!')\n",
    "            return\n",
    "        \n",
    "        if self.is_offscreen():\n",
    "            print('The car or trailer is off screen')\n",
    "            return\n",
    "        \n",
    "        self.ϕ = ϕ\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        \n",
    "        # Perform state update\n",
    "        self.x += s * cos(θ0) * dt\n",
    "        self.y += s * sin(θ0) * dt\n",
    "        self.θ0 += s / L * tan(ϕ) * dt\n",
    "        self.θ1 += s / d * sin(θ0 - θ1) * dt\n",
    "        \n",
    "        return (self.x, self.y, self.θ0, *self._traler_xy(), self.θ1)\n",
    "    \n",
    "    def state(self):\n",
    "        return (self.x, self.y, self.θ0, *self._traler_xy(), self.θ1)\n",
    "    \n",
    "    def _get_atributes(self):\n",
    "        return (\n",
    "            self.x, self.y, self.W, self.L, self.d, self.s,\n",
    "            self.θ0, self.θ1, self.ϕ\n",
    "        )\n",
    "    \n",
    "    def _traler_xy(self):\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        return x - d * cos(θ1), y - d * sin(θ1)\n",
    "        \n",
    "    def is_jackknifed(self):\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        return abs(θ0 - θ1) * 180 / π > 90\n",
    "    \n",
    "    def is_offscreen(self):\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        \n",
    "        x1, y1 = x + 1.5 * L * cos(θ0), y + 1.5 * L * sin(θ0)\n",
    "        x2, y2 = self._traler_xy()\n",
    "        \n",
    "        b = self.box\n",
    "        return not (\n",
    "            b[0] <= x1 <= b[1] and b[2] <= y1 <= b[3] and\n",
    "            b[0] <= x2 <= b[1] and b[2] <= y2 <= b[3]\n",
    "        )\n",
    "        \n",
    "    def valid(self):\n",
    "        return not self.is_jackknifed() and not self.is_offscreen()\n",
    "        \n",
    "    def draw(self):\n",
    "        if not self.display: return\n",
    "        if self.patches: self.clear()\n",
    "        self._draw_car()\n",
    "        self._draw_trailer()\n",
    "        self.f.canvas.draw()\n",
    "            \n",
    "    def clear(self):\n",
    "        for p in self.patches:\n",
    "            p.remove()\n",
    "        self.patches = list()\n",
    "        \n",
    "    def _draw_car(self):\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        ax = self.ax\n",
    "        \n",
    "        x1, y1 = x + L / 2 * cos(θ0), y + L / 2 * sin(θ0)\n",
    "        bar = Line2D((x, x1), (y, y1), lw=5, color='C2', alpha=0.8)\n",
    "        ax.add_line(bar)\n",
    "\n",
    "        car = Rectangle(\n",
    "            (x1, y1 - W / 2), L, W, angle=0, color='C2', alpha=0.8, transform=\n",
    "            matplotlib.transforms.Affine2D().rotate_deg_around(x1, y1, θ0 * 180 / π) +\n",
    "            ax.transData\n",
    "        )\n",
    "        ax.add_patch(car)\n",
    "\n",
    "        x2, y2 = x1 + L / 2 ** 0.5 * cos(θ0 + π / 4), y1 + L / 2 ** 0.5 * sin(θ0 + π / 4)\n",
    "        left_wheel = Line2D(\n",
    "            (x2 - L / 4 * cos(θ0 + ϕ), x2 + L / 4 * cos(θ0 + ϕ)),\n",
    "            (y2 - L / 4 * sin(θ0 + ϕ), y2 + L / 4 * sin(θ0 + ϕ)),\n",
    "            lw=3, color='C5', alpha=1)\n",
    "        ax.add_line(left_wheel)\n",
    "\n",
    "        x3, y3 = x1 + L / 2 ** 0.5 * cos(π / 4 - θ0), y1 - L / 2 ** 0.5 * sin(π / 4 - θ0)\n",
    "        right_wheel = Line2D(\n",
    "            (x3 - L / 4 * cos(θ0 + ϕ), x3 + L / 4 * cos(θ0 + ϕ)),\n",
    "            (y3 - L / 4 * sin(θ0 + ϕ), y3 + L / 4 * sin(θ0 + ϕ)),\n",
    "            lw=3, color='C5', alpha=1)\n",
    "        ax.add_line(right_wheel)\n",
    "        \n",
    "        self.patches += [car, bar, left_wheel, right_wheel]\n",
    "        \n",
    "    def _draw_trailer(self):\n",
    "        x, y, W, L, d, s, θ0, θ1, ϕ = self._get_atributes()\n",
    "        ax = self.ax\n",
    "            \n",
    "        x, y = x - d * cos(θ1), y - d * sin(θ1) - W / 2\n",
    "        trailer = Rectangle(\n",
    "            (x, y), d, W, angle=0, color='C0', alpha=0.8, transform=\n",
    "            matplotlib.transforms.Affine2D().rotate_deg_around(x, y + W/2, θ1 * 180 / π) +\n",
    "            ax.transData\n",
    "        )\n",
    "        ax.add_patch(trailer)\n",
    "        \n",
    "        self.patches += [trailer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck = Truck(display=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ϕ = deg2rad(-35)  # positive left, negative right\n",
    "truck.step(ϕ)\n",
    "truck.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truck.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build expert data set\n",
    "\n",
    "episodes = 10\n",
    "inputs = list()\n",
    "outputs = list()\n",
    "truck = Truck(); episodes = 10_000  # uncooment for creating the data set\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    \n",
    "    truck.reset()\n",
    "    \n",
    "    while truck.valid():\n",
    "        initial_state = truck.state()\n",
    "        ϕ = (random() - 0.5) * π / 2\n",
    "        inputs.append((ϕ, *initial_state))\n",
    "        outputs.append(truck.step(ϕ))\n",
    "        truck.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 6\n",
    "steering_size = 1\n",
    "hidden_units_e = 45\n",
    "\n",
    "emulator = nn.Sequential(\n",
    "    nn.Linear(steering_size + state_size, hidden_units_e),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_units_e, state_size)\n",
    ")\n",
    "\n",
    "optimiser_e = SGD(emulator.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_inputs = torch.Tensor(inputs)\n",
    "tensor_outputs = torch.Tensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = tensor_inputs.mean(0)\n",
    "std = tensor_inputs.std(0)\n",
    "tensor_inputs = (tensor_inputs - mean) / std\n",
    "tensor_outputs = (tensor_outputs - mean[1:]) / std[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80:20 for test:train.\n",
    "test_size = int(len(tensor_inputs) * 0.8)\n",
    "print(len(tensor_inputs), test_size)\n",
    "\n",
    "train_inputs = tensor_inputs[:test_size]\n",
    "train_outputs = tensor_outputs[:test_size]\n",
    "test_inputs = tensor_inputs[test_size:]\n",
    "test_outputs = tensor_outputs[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emulator training\n",
    "cnt = 0\n",
    "for i in torch.randperm(len(train_inputs)):\n",
    "    ϕ_state = train_inputs[i]\n",
    "    next_state_prediction = emulator(ϕ_state)\n",
    "    \n",
    "    next_state = train_outputs[i]\n",
    "    loss = criterion(next_state_prediction, next_state)\n",
    "    \n",
    "    optimiser_e.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_e.step()\n",
    "    \n",
    "    if cnt == 0 or (cnt + 1) % 1000 == 0:\n",
    "        print(f'{cnt + 1:4d} / {len(train_inputs)}, {loss.item():.10f}', end='\\r')\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for idx, ϕ_state in enumerate(test_inputs):\n",
    "        next_state_prediction = emulator(ϕ_state)\n",
    "\n",
    "        next_state = test_outputs[idx]\n",
    "        total_loss += criterion(next_state_prediction, next_state).item()\n",
    "\n",
    "ave_test_loss = total_loss/test_size\n",
    "print(f'Test loss: {ave_test_loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze emulator\n",
    "for param in emulator.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pick_action(controller, state, epsilon):\n",
    "#     if torch.rand(1) < epsilon:\n",
    "#         return torch.rand(1)\n",
    "#     # FIXME: the model returns the expected total reward for each action.\n",
    "#     # We need to return the action with the highest expected reward.\n",
    "#     return controller(state)\n",
    "\n",
    "\n",
    "# # Unit tests for take_action\n",
    "# controller = lambda s: 2.0\n",
    "# action_random = pick_action(controller, state=(1, 2, 3), epsilon=1.0)\n",
    "# assert 0 <= action_random < 1, f\"{action_random} not in range [0, 1)\"\n",
    "# action_controller = pick_action(controller, state=(1, 2, 3), epsilon=0.0)\n",
    "# assert action_controller == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def episode(truck, emulator, controller, max_steps=100):\n",
    "#     truck.reset()\n",
    "#     steps = 0\n",
    "#     while truck.valid() and steps < max_steps:\n",
    "#         steering_angle = controller(torch.tensor(\n",
    "#             truck.state(), requires_grad=True\n",
    "#         ).to(torch.float))\n",
    "#         # controller output varies between -1 and 1.\n",
    "#         # Scale that to +/- pi/4.\n",
    "#         steering_angle_rad = steering_angle * pi / 4\n",
    "#         emulator_input = torch.tensor(\n",
    "#             truck.state() + (steering_angle,),\n",
    "#         ).to(torch.float)\n",
    "#         with torch.no_grad():\n",
    "#             emulated_state = emulator(emulator_input).detach()\n",
    "#         truck.x = emulated_state[0]\n",
    "#         truck.y = emulated_state[1]\n",
    "#         truck.θ0 = emulated_state[2]\n",
    "#         truck.θ1 = emulated_state[5]\n",
    "#         steps += 1\n",
    "#     return truck.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actor picks an action (steering angle) from a normal distribution.\n",
    "# The mean and standard deviation of the distribution are learned.\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "        hidden_input = state_size\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        for hidden_output in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(hidden_input, hidden_output))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.LayerNorm(hidden_output))\n",
    "            hidden_input = hidden_output\n",
    "        self.mean_layer = nn.Linear(hidden_output, 1)\n",
    "        self.std_layer = nn.Linear(hidden_output, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden = self.layers(state)\n",
    "        # Apply tanh so mean varies from -1 to 1\n",
    "        mean = torch.tanh(self.mean_layer(hidden))\n",
    "        # Apply sigmoid so STD varies from 0 to 1\n",
    "        std = torch.sigmoid(self.std_layer(hidden))\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critic estimates the future reward from the given state.\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "        hidden_input = state_size\n",
    "        self.layers = torch.nn.Sequential()\n",
    "        for hidden_output in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(hidden_input, hidden_output))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.LayerNorm(hidden_output))\n",
    "            hidden_input = hidden_output\n",
    "        self.layers.append(torch.nn.Linear(hidden_output, 1))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ActorCritic(nn.Module):\n",
    "#     def __init__(self, state_size, hidden_sizes):\n",
    "#         super().__init__()\n",
    "#         hidden_input = state_size\n",
    "#         self.hidden_layers = torch.nn.Sequential()\n",
    "#         for hidden_output in hidden_sizes:\n",
    "#             self.hidden_layers.append(nn.Linear(hidden_input, hidden_output))\n",
    "#             self.hidden_layers.append(torch.nn.ReLU())\n",
    "#             self.hidden_layers.append(torch.nn.LayerNorm(hidden_output))\n",
    "#             hidden_input = hidden_output\n",
    "#         self.actor_mean = nn.Linear(hidden_output, 1)\n",
    "#         self.actor_std = nn.Linear(hidden_output, 1)\n",
    "#         self.critic = nn.Linear(hidden_output, 1)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         hidden = self.hidden_layers(state)\n",
    "#         # Apply tanh so mean varies from -1 to 1\n",
    "#         mean = torch.tanh(self.actor_mean(hidden))\n",
    "#         # Apply sigmoid so STD varies from 0 to 1\n",
    "#         std = torch.sigmoid(self.actor_std(hidden))\n",
    "#         value = self.critic(hidden)\n",
    "#         return mean, std, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_state = torch.tensor([\n",
    "    0.,  # trailer_x\n",
    "    0.,  # trailer_y\n",
    "    0.,  # trailer_theta\n",
    "])\n",
    "worst_state = torch.tensor([\n",
    "    max(abs(x) for x in ENVIRONMENT_BBOX[:2]),\n",
    "    max(abs(x) for x in ENVIRONMENT_BBOX[2:]),\n",
    "    STEERING_ANGLE_RANGE_rad,\n",
    "])\n",
    "# Use smooth L1 (~Huber) loss instead of MSE to avoid exploding gradients\n",
    "loss_func = torch.nn.SmoothL1Loss(reduction=\"none\")\n",
    "worst_error = loss_func(desired_state, worst_state).mean()\n",
    "\n",
    "def compute_reward(truck_state):\n",
    "    \"\"\" Calculate the reward for the given truck state(s)\n",
    "\n",
    "    truck_state is a 1D tensor of size 6 or a 2D tensor of shape Bx6, where B is\n",
    "    the batch size.\n",
    "    \n",
    "    The output is a 1D tensor of size B. If the input tensor is 1D, the output\n",
    "    is a 1D tensor of size 1 (i.e. B is considered to be 1).\n",
    "    \"\"\"\n",
    "    if truck_state.ndim < 2:\n",
    "        truck_state = truck_state.unsqueeze(0)\n",
    "    relevant_truck_state = truck_state[:, 3:]\n",
    "    state_error = loss_func(\n",
    "        desired_state.repeat(truck_state.size(0), 1),\n",
    "        relevant_truck_state,\n",
    "    ).mean(dim=1)\n",
    "    # Subtract current loss from worst possible\n",
    "    # so being close to the goal produces a high reward\n",
    "    return worst_error - state_error\n",
    "\n",
    "# def compute_reward(truck_state):\n",
    "#     relevant_truck_state = truck_state[3:]\n",
    "#     return -loss_func(desired_state, relevant_truck_state)\n",
    "\n",
    "\n",
    "# Unit tests for compute_reward()\n",
    "def test_reward(test_state, expected_reward):\n",
    "    reward = compute_reward(test_state)\n",
    "    assert torch.allclose(reward, expected_reward), (\n",
    "        f\"{reward=} != {expected_reward=}\"\n",
    "    )\n",
    "\n",
    "test_reward(torch.zeros(state_size), worst_error)\n",
    "test_reward(torch.zeros(4, state_size), worst_error.repeat(4))\n",
    "\n",
    "test_state = torch.cat((torch.zeros(3), worst_state))\n",
    "test_reward(test_state, torch.tensor(0.0))\n",
    "\n",
    "test_state = torch.tensor((0, 0, 1, 0, 0, 0))\n",
    "test_reward(test_state, worst_error)\n",
    "\n",
    "test_state = torch.tensor((0, 0, 0, 1, 0, 0))\n",
    "test_reward(test_state, worst_error - 1/6)\n",
    "\n",
    "test_state = torch.tensor((0, 0, 0, 1, 1, 0))\n",
    "test_reward(test_state, worst_error - 1/3)\n",
    "\n",
    "test_state = torch.tensor((0, 0, 0, 2, 1, 0))\n",
    "test_reward(test_state, worst_error - 2/3)\n",
    "\n",
    "test_state = torch.tensor((0, 0, 0, 2, -1, -3))\n",
    "test_reward(test_state, worst_error - 3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same math as the Truck class but in a batch-friendly, functional form rather\n",
    "# than an object-oriented one\n",
    "\n",
    "# Constants copied from Truck class\n",
    "W = 1  # car and trailer width\n",
    "L = 1 * W  # car length\n",
    "d = 4 * L\n",
    "s = -0.1\n",
    "dt = 1\n",
    "\n",
    "\n",
    "def random_truck_state(batch_size):\n",
    "    \"\"\" Return a Bx6 tensor of random truck states \"\"\"\n",
    "    state = torch.rand(batch_size, state_size)\n",
    "    # The positional ranges are adjusted slightly compared to the Truck class to\n",
    "    # ensure the state is always valid\n",
    "    state[:, 0] = (state[:, 0] * 0.75 + 0.25) * (ENVIRONMENT_BBOX[1] - d)  # x0\n",
    "    state[:, 1] = (state[:, 1] - 0.5) * (  # y0\n",
    "        ENVIRONMENT_BBOX[3] - ENVIRONMENT_BBOX[2] - 2 * d\n",
    "    )\n",
    "    state[:, 2] *= 2 * π  # 0 <= θ0 < 2π\n",
    "    state[:, 5] = (state[:, 5] - 0.5) * π/2 + state[:, 2]  # -π/4 <= θ1 - θ0 < π/4\n",
    "    state[:, 3] = state[:, 0] - d * torch.cos(state[:, 5])  # x1\n",
    "    state[:, 4] = state[:, 1] - d * torch.sin(state[:, 5])  # y1\n",
    "    return state\n",
    "\n",
    "\n",
    "def is_valid(truck_state):\n",
    "    \"\"\" Calculate whether the given truck state(s) is/are valid\n",
    "\n",
    "    truck_state is a 1D tensor of size 6 or a 2D tensor of shape Bx6, where B is\n",
    "    the batch size.\n",
    "    \n",
    "    The output is a 1D, boolean tensor of size B. If the input tensor is 1D, the\n",
    "    output is a 1D tensor of size 1 (i.e. B is considered to be 1).\n",
    "    \"\"\"\n",
    "    if truck_state.ndim < 2:\n",
    "        truck_state = truck_state.unsqueeze(0)\n",
    "    # Make the various positions and angles the first dimension so it's easier\n",
    "    # to split them into separate tensors.\n",
    "    truck_state = truck_state.transpose(0, 1)\n",
    "    (x0, y0, θ0, trailer_x, trailer_y, θ1) = truck_state\n",
    "    jackknifed = torch.abs(θ0 - θ1) > π / 2\n",
    "\n",
    "    cab_x = x0 + 1.5 * L * torch.cos(θ0)\n",
    "    cab_y = y0 + 1.5 * L * torch.sin(θ0)\n",
    "    bbox = ENVIRONMENT_BBOX\n",
    "    # pytorch doesn't like to do a < x < b, so we have to do (a < x) & (x < b)\n",
    "    in_bounds = (\n",
    "        (bbox[0] <= cab_x) & (cab_x <= bbox[1])\n",
    "        & (bbox[2] <= cab_y) & (cab_y <= bbox[3])\n",
    "        & (bbox[0] <= trailer_x) & (trailer_x <= bbox[1])\n",
    "        & (bbox[2] <= trailer_y) & (trailer_y <= bbox[3])\n",
    "    )\n",
    "    return ~jackknifed & in_bounds\n",
    "    \n",
    "\n",
    "def analytical_world_model(steering_angle_and_state):\n",
    "    \"\"\" Update the environment by taking a single step\n",
    "    \n",
    "    steering_angle_and_state is a 1D tensor of size 7 or a 2D tensor of shape\n",
    "    Bx7, where B is the batch size. It represents the desired steering angle and\n",
    "    the current truck state.\n",
    "    \n",
    "    The output is a 1D tensor of size 6 or a 2D tensor of shape Bx6 representing\n",
    "    the new truck state.\n",
    "    \"\"\"\n",
    "    if steering_angle_and_state.ndim == 1:\n",
    "        steering_angle_and_state = steering_angle_and_state.unsqueeze(0)\n",
    "    # Make the various positions and angles the first dimension so it's easier\n",
    "    # to split them into separate tensors.\n",
    "    steering_angle_and_state = steering_angle_and_state.transpose(0, 1)\n",
    "    (ϕ, x0, y0, θ0, x1, y1, θ1) = steering_angle_and_state\n",
    "    x0_ = x0 + s * torch.cos(θ0) * dt\n",
    "    y0_ = y0 + s * torch.sin(θ0) * dt\n",
    "    θ1_ = θ1 + s / d * sin(θ0 - θ1) * dt\n",
    "    θ0_ = θ0 + s / L * tan(ϕ) * dt\n",
    "    x1_ = x0 - d * torch.cos(θ1)\n",
    "    y1_ = y0 - d * torch.sin(θ1)\n",
    "    return torch.stack((x0_, y0_, θ0_, x1_, y1_, θ1_), dim=1)\n",
    "\n",
    "\n",
    "# Unit tests for analytical_world_model\n",
    "def test_world_model(starting_state, expected_next_state):\n",
    "    next_state = analytical_world_model(starting_state)\n",
    "    assert torch.allclose(\n",
    "        torch.as_tensor(next_state), torch.as_tensor(expected_next_state)), (\n",
    "        f\"{next_state=} != {expected_next_state=}\"\n",
    "    )\n",
    "\n",
    "starting_state = torch.tensor([0.0, 0.0, 0.0, 0.0, -3.9, 0.0, 0.0])\n",
    "expected_next_state = torch.tensor([-0.1, 0.0, 0.0, -4.0, 0.0, 0.0])\n",
    "test_world_model(starting_state, expected_next_state)\n",
    "\n",
    "starting_state = starting_state.unsqueeze(0).repeat(16, 1)\n",
    "expected_next_state = expected_next_state.unsqueeze(0).repeat(16, 1)\n",
    "test_world_model(starting_state, expected_next_state)\n",
    "\n",
    "# Test that random starting states are always valid\n",
    "random_states = random_truck_state(1000)\n",
    "invalid = ~is_valid(random_states)\n",
    "invalid_count = torch.count_nonzero(invalid).item()\n",
    "assert invalid_count == 0, f\"{invalid_count=}\\n{random_states[invalid]=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: (epochs, batch_size, training_time)\n",
    "training_times = torch.zeros((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you need to insert the code for training the controller\n",
    "# by using the emulator for backpropagation\n",
    "\n",
    "# If you succeed, feel free to send a PR\n",
    "\n",
    "# Things to try\n",
    "#   RL algorithms\n",
    "#       Q-learning: requires discrete action space?\n",
    "#       Q actor-critic\n",
    "#       Advantage actor-critic\n",
    "#   update every step instead of at end of episode\n",
    "#   explore/exploit: take random action with decreasing probability\n",
    "#   save state transitions to memory and take batches from memory\n",
    "#       better suited to critic since it's basically Q learner?\n",
    "#   prevent actor from being over-confident. add entropy metric to loss.\n",
    "#   model params become NaN\n",
    "#       learning rate\n",
    "#       smaller loss/reward\n",
    "#       normalize\n",
    "#   low episode length limit to start, increase as model trains\n",
    "#   compare episode losses to number of steps\n",
    "#   different discount factors and episode length limits\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "NUM_EPOCHS = int(2**14)\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "TIME_DISCOUNT = 0.95\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "actor = Actor(state_size, hidden_sizes=[25, 50])\n",
    "critic = Critic(state_size, hidden_sizes=[25, 50])\n",
    "actor_opt = torch.optim.Adam(actor.parameters(), lr=1e-6)\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=1e-2)\n",
    "\n",
    "world_model = analytical_world_model  # Use exact world model\n",
    "# world_model = emulator  # Use trained emulator\n",
    "\n",
    "NUM_LOSSES_TO_PLOT = 100\n",
    "log_every_n_epochs = max(NUM_EPOCHS // NUM_LOSSES_TO_PLOT, 1)\n",
    "losses_to_plot = []\n",
    "entropies_to_plot = []\n",
    "NUM_EPISODES_TO_PLOT = 25\n",
    "longest_episodes = torch.full(\n",
    "    (NUM_EPISODES_TO_PLOT, MAX_STEPS_PER_EPISODE, state_size),\n",
    "    torch.nan,\n",
    "    dtype=torch.float,\n",
    ")\n",
    "longest_lengths = torch.zeros(NUM_EPISODES_TO_PLOT, dtype=torch.int)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    current_state = random_truck_state(BATCH_SIZE)\n",
    "    # Store actor and critic outputs separately since pytorch complains about\n",
    "    # visiting the same tensor twice when backpropagating (even though the\n",
    "    # relevant elements are different).\n",
    "    # Tensor of action log probabilities\n",
    "    episode_actor = torch.zeros(\n",
    "        (BATCH_SIZE, MAX_STEPS_PER_EPISODE), dtype=torch.float\n",
    "    )\n",
    "    # Tensor of critic values for each state\n",
    "    episode_critic = torch.zeros_like(episode_actor)\n",
    "    # Tensor of (reward, done)\n",
    "    episode_reward_done = torch.zeros(\n",
    "        (BATCH_SIZE, MAX_STEPS_PER_EPISODE, 2), dtype=torch.float\n",
    "    )\n",
    "    # Overwrite steps from shortest saved episode\n",
    "    saved_episodes_index = torch.min(longest_lengths, dim=0).indices.item()\n",
    "    longest_episodes[saved_episodes_index, :, :] = torch.nan\n",
    "\n",
    "    for step in range(MAX_STEPS_PER_EPISODE):\n",
    "        # Only consider the first episode in the batch. Could instead consider\n",
    "        # the longest episode in the batch, but that's more complicated without\n",
    "        # much practical benefit.\n",
    "        longest_episodes[saved_episodes_index, step, :] = current_state[0,:]\n",
    "\n",
    "        action_mean, action_std = actor(current_state)\n",
    "        action_mean = action_mean\n",
    "        action_std = action_std\n",
    "        critic_value = critic(current_state).squeeze(1)\n",
    "        action_distribution = torch.distributions.normal.Normal(\n",
    "            action_mean, action_std\n",
    "        )\n",
    "        action_sample = action_distribution.rsample()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            steering_angle = STEERING_ANGLE_RANGE_rad * action_sample\n",
    "            world_model_input = torch.cat((steering_angle, current_state), dim=1)\n",
    "            assert world_model_input.shape == (\n",
    "                BATCH_SIZE, steering_size + state_size\n",
    "            )\n",
    "            next_state = world_model(world_model_input)\n",
    "            reward = compute_reward(next_state)\n",
    "\n",
    "        episode_actor[:, step] = action_distribution.log_prob(\n",
    "            action_sample\n",
    "        ).squeeze(1)\n",
    "        # These values done need to be calculated on each iteration. They could\n",
    "        # be calculated after the episode is over. Would that be faster?\n",
    "        episode_critic[:, step] = critic_value\n",
    "        episode_reward_done[:, step, 0] = reward\n",
    "        episode_reward_done[:, step, 1] = ~is_valid(current_state)\n",
    "        if torch.all(episode_reward_done[:, step, 1]):\n",
    "            # All states in batch invalid, break out early\n",
    "            episode_actor = episode_actor[:, :step + 1]\n",
    "            episode_critic = episode_critic[:, :step + 1]\n",
    "            episode_reward_done = episode_reward_done[:, :step + 1, :]\n",
    "            break\n",
    "        current_state = next_state\n",
    "\n",
    "    # Episode over. Calculate return (discounted total reward) and loss.\n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros(\n",
    "            (BATCH_SIZE, episode_reward_done.size(1)), dtype=float\n",
    "        )\n",
    "        # Use critic to estimate remaining reward\n",
    "        next_return = critic(next_state).detach().squeeze(1)\n",
    "        for j in range(returns.size(1) - 1, -1, -1):\n",
    "            instant_reward = episode_reward_done[:, j, 0]\n",
    "            done = episode_reward_done[:, j, 1].unsqueeze(1)\n",
    "            next_return = (instant_reward + TIME_DISCOUNT * next_return)\n",
    "            returns[:, j] = next_return\n",
    "            # Zero out any reward following an invalid state\n",
    "            returns[:, j:] *= 1.0 - done\n",
    "\n",
    "    advantage = returns - episode_critic\n",
    "\n",
    "    actor_loss = (-episode_actor * advantage.detach()).mean()\n",
    "    # Equivalent to RMS error between critic value and return\n",
    "    critic_loss = torch.sqrt((advantage * advantage).mean())\n",
    "\n",
    "    actor_opt.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_opt.step()\n",
    "    critic_opt.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_opt.step()\n",
    "\n",
    "    if i % log_every_n_epochs == 0:\n",
    "        print(\n",
    "            f\"Epoch {i:5d}:\",\n",
    "            f\"length {step:3d}\",\n",
    "            f\"actor loss {actor_loss.item():7.3f},\",\n",
    "            f\"critic loss {critic_loss.item():7.3f}\",\n",
    "            end=\"\\r\",\n",
    "        )\n",
    "        losses_to_plot.append((i, actor_loss.item(), critic_loss.item()))\n",
    "        entropies_to_plot.append((i, action_distribution.entropy().mean().item()))\n",
    "\n",
    "    longest_lengths[saved_episodes_index] = step\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "training_time = stop_time - start_time\n",
    "print(f\"\\nTraining took {training_time:.3f} seconds\")\n",
    "training_times = torch.cat(\n",
    "    (training_times, torch.tensor([[NUM_EPOCHS, BATCH_SIZE, training_time]]))\n",
    ")\n",
    "\n",
    "losses_to_plot = torch.as_tensor(losses_to_plot)\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.plot(losses_to_plot[:, 0], losses_to_plot[:, 1], color=\"C0\", label=\"Actor\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Actor Loss\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(losses_to_plot[:, 0], losses_to_plot[:, 2], color=\"C1\", label=\"Critic\")\n",
    "ax2.set_label(\"Critic Loss\")\n",
    "ax2.legend(handles=ax1.lines + ax2.lines, labelcolor=\"black\")\n",
    "ax2.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average filter by convolving with window of 1s\n",
    "filter_window = 11\n",
    "with torch.no_grad():\n",
    "    filtered_loss = torch.nn.functional.conv1d(\n",
    "        losses_to_plot[:, 1:].T,\n",
    "        weight=torch.ones((2, 1, filter_window)),\n",
    "        groups=2,\n",
    "    ).T / filter_window\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.plot(\n",
    "    losses_to_plot[:-filter_window + 1, 0],\n",
    "    filtered_loss[:, 0],\n",
    "    color=\"C0\",\n",
    "    label=\"Actor\",\n",
    ")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Actor Loss\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    losses_to_plot[:-filter_window + 1, 0],\n",
    "    filtered_loss[:, 1],\n",
    "    color=\"C1\",\n",
    "    label=\"Critic\",\n",
    ")\n",
    "ax2.set_label(\"Critic Loss\")\n",
    "ax2.legend(handles=ax1.lines + ax2.lines, labelcolor=\"black\")\n",
    "ax2.grid(False)\n",
    "ax1.set_title(\"Filtered Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss = losses_to_plot[:, 1:].sum(1)\n",
    "filtered_combined_loss = filtered_loss.sum(1)\n",
    "len_diff = len(combined_loss) - len(filtered_combined_loss)\n",
    "plt.plot(\n",
    "    losses_to_plot[:, 0],\n",
    "    combined_loss,\n",
    "    label=\"Combined Loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    losses_to_plot[len_diff // 2 : len_diff // 2 + len(filtered_combined_loss), 0],\n",
    "    filtered_combined_loss,\n",
    "    label=\"Filtered Combined Loss\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(labelcolor=\"Black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_to_plot = torch.as_tensor(entropies_to_plot)\n",
    "plt.plot(entropies_to_plot[:, 0], entropies_to_plot[:, 1])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Action Distribution Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode(states, axis=None):\n",
    "    states = states.numpy()\n",
    "    if axis is None:\n",
    "        _, axis = plt.subplots()\n",
    "    axis.scatter(\n",
    "        states[:,0], states[:,1],\n",
    "        # Color points according to order\n",
    "        c=np.arange(len(states)),\n",
    "        vmin=0, vmax=MAX_STEPS_PER_EPISODE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(longest_episodes.shape)\n",
    "print(longest_lengths)\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=5, ncols=5, sharex=True, sharey=True, figsize=(12, 8),\n",
    ")\n",
    "for k, ax in enumerate(axes.flatten()):\n",
    "    plot_episode(longest_episodes[k], axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

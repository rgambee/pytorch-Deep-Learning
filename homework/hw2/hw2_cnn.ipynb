{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYszeLBT-8AL"
   },
   "source": [
    "# Homework 2 - Convolutional Neural Nets\n",
    "\n",
    "In this homework, we will be working with google [colab](https://colab.research.google.com/). Google colab allows you to run a jupyter notebook on google servers using a GPU or TPU. To enable GPU support, make sure to press Runtime -> Change Runtime Type -> GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLI0m6U7_lZt"
   },
   "source": [
    "## Cats vs dogs classification\n",
    "\n",
    "To learn about and experiment with convolutional neural nets we will be working on a problem of great importance in computer vision - classifying images of cats and dogs.\n",
    "\n",
    "The problem is so important that there's even an easter egg in colab: go to Tools -> Settings -> Miscellaneous and enable 'Corgi mode' and 'Kitty mode' to get more cats and dogs to classify when you're tired of coding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtsy_bpEC8wp"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "To get started with the classification, we first need to download and unpack the dataset (note that in jupyter notebooks commands starting with `!` are executed in bash, not in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My247KXGJIHe"
   },
   "outputs": [],
   "source": [
    "! wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O ./cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Wn0Tw69J9FR"
   },
   "outputs": [],
   "source": [
    "! unzip cats_and_dogs_filtered.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGQA7kgUAHw0"
   },
   "source": [
    "This dataset contains two directories, `train` and `validation`. Both in turn contain two directories with images: `cats` and `dogs`. In `train` we have 1000 images of cats, and another 1000 images of dogs. For `validation`, we have 500 images of each class. Our goal is to implement and train a convolutional neural net to classify these images, i.e. given an image from this dataset, tell if it contains a cat or a dog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yqS2szPCVRH"
   },
   "outputs": [],
   "source": [
    "! echo 'Training cats examples:' $(find cats_and_dogs_filtered/train/cats -type f | wc -l)\n",
    "! echo 'Training dogs examples:' $(find cats_and_dogs_filtered/train/dogs -type f | wc -l)\n",
    "! echo 'Validation cats examples:' $(find cats_and_dogs_filtered/validation/cats -type f | wc -l)\n",
    "! echo 'Validation dogs examples:' $(find cats_and_dogs_filtered/validation/dogs -type f | wc -l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr8YteOYC4Da"
   },
   "source": [
    "### Loading the data\n",
    " Now that we have the data on our disk, we need to load it so that we can use it to train our model. In Pytorch ecosystem, we use `Dataset` class, documentation for which can be found [here](https://pytorch.org/docs/stable/data.html). \n",
    "\n",
    " In the case of computer vision, the datasets with the folder structure 'label_name/image_file' are very common, and to process those there's already a class `torchvision.datasets.ImageFolder` (documented [here](https://pytorch.org/vision/0.8/datasets.html)). Torchvision is a Pytorch library with many commonly used tools in computer vision.\n",
    "\n",
    " Another thing we need from Torchvision library is transforms ([documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)). In computer vision, we very often want to transform the images in certain ways. The most common is normalization. Others include flipping, changing saturation, hue, contrast, rotation, and blurring. \n",
    "\n",
    " Below, we create a training, validation and test sets. We use a few transforms for augmentation on the training set, but we don't use anything but resize and normalization for validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5nDSr1LLA1s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image # PIL is a library to process images\n",
    "\n",
    "# These numbers are mean and std values for channels of natural images. \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Inverse transformation: needed for plotting.\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(hue=.1, saturation=.1, contrast=.1),\n",
    "    transforms.RandomRotation(20, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.GaussianBlur(7, sigma=(0.1, 1.0)),\n",
    "    transforms.ToTensor(),  # convert PIL to Pytorch Tensor\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(), \n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    './cats_and_dogs_filtered/train/', transform=train_transforms\n",
    ")\n",
    "validation_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    torchvision.datasets.ImageFolder('./cats_and_dogs_filtered/validation/',\n",
    "                                     transform=validation_transforms),\n",
    "    [500, 500],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM4wsBQxFOh-"
   },
   "source": [
    "Let's see what one of the images in the dataset looks like (you can run this cell multiple times to see the effects of different augmentations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvurMzqGLUnX"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200 # change dpi to make plots bigger\n",
    "\n",
    "def show_normalized_image(img, title=None):\n",
    "    plt.imshow(unnormalize(img).detach().cpu().permute(1, 2, 0))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# show_normalized_image(train_dataset[10][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8MytjMIFl18"
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGPgMBLqFpME"
   },
   "source": [
    "Now is the time to create a model. All models in Pytorch are subclassing `torch.nn.Module`, and have to implement `__init__` and `forward` methods. \n",
    "\n",
    "Below we provide a simple model skeleton, which you need to expand. The places to put your code are marked with `TODO`. Here, we ask you to implement a convolutional neural network containing the following elements:\n",
    "\n",
    "* Convolutional layers (at least two)\n",
    "* Batch Norm\n",
    "* Non-linearity\n",
    "* Pooling layers\n",
    "* A residual connection similar to that of Res-Net\n",
    "* A fully connected layer\n",
    "\n",
    "For some examples of how to implement Pytorch models, please refer to our lab notebooks, such as [this one](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmt1CitJM6xD"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Other ideas to try:\n",
    "#    More internal channels\n",
    "#    Dropout\n",
    "#    More convolutional and/or linear layers\n",
    "\n",
    "CNN = torch.nn.Sequential(\n",
    "    # 3x256x256\n",
    "    nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1),\n",
    "    # 3x253x253\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=5, stride=3),\n",
    "    # 3x83x83\n",
    "    nn.BatchNorm2d(3, affine=True),\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=2),\n",
    "    # 16x40x40\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=5, stride=4),\n",
    "    # 1x9x9\n",
    "    nn.BatchNorm2d(16, affine=True),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=9, stride=1),\n",
    "    nn.ReLU(),\n",
    "    # 32x1x1\n",
    "    nn.Flatten(),\n",
    "    # Use 1 output neuron since this is a\n",
    "    # binary classification task\n",
    "    nn.Linear(32, 1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEQpspPpHCsE"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now we train the model on the dataset. Again, we're providing you with the skeleton with some parts marked as `TODO` to be filled by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdJT_iYvOTtS"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_loss_and_correct(model, batch, criterion, device):\n",
    "    # Implement forward pass and loss calculation for one batch.\n",
    "    # Remember to move the batch to device.\n",
    "    # \n",
    "    # Return a tuple:\n",
    "    # - loss for the batch (Tensor)\n",
    "    # - number of correctly classified examples in the batch (Tensor)\n",
    "    images, target = batch\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "    prediction_raw = model(images).ravel()\n",
    "    assert prediction_raw.size() == target.size(), f'{prediction_raw.size()=} != {target.size()=}'\n",
    "    prediction_bool = prediction_raw > 0\n",
    "    loss = criterion(prediction_raw, target.to(prediction_raw))\n",
    "    correct = prediction_bool == target\n",
    "    num_correct = correct.count_nonzero()\n",
    "    return loss, num_correct\n",
    "  \n",
    "\n",
    "def step(loss, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 50\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "model = CNN\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-3)\n",
    "\n",
    "model.train()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS))\n",
    "\n",
    "for i in pbar:\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0.0\n",
    "    total_validation_loss = 0.0\n",
    "    total_validation_correct = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, leave=False):\n",
    "        loss, correct = get_loss_and_correct(model, batch, criterion, device)\n",
    "        step(loss, optimizer)\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_correct += correct.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            loss, correct = get_loss_and_correct(model, batch, criterion, device)\n",
    "            total_validation_loss += loss.item()\n",
    "            total_validation_correct += correct.item()\n",
    "\n",
    "    mean_train_loss = total_train_loss / len(train_dataset)\n",
    "    train_accuracy = total_train_correct / len(train_dataset)\n",
    "\n",
    "    mean_validation_loss = total_validation_loss / len(validation_dataset)\n",
    "    validation_accuracy = total_validation_correct / len(validation_dataset)\n",
    "\n",
    "    train_losses.append(mean_train_loss)\n",
    "    validation_losses.append(mean_validation_loss)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "    pbar.set_postfix({'train_loss': mean_train_loss, 'validation_loss': mean_validation_loss, 'train_accuracy': train_accuracy, 'validation_accuracy': validation_accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZPa_-HH9S3"
   },
   "source": [
    "Now that the model is trained, we want to visualize the training and validation losses and accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYJtDFiNxCCj"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(validation_losses, label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_accuracies, label='train')\n",
    "plt.plot(validation_accuracies, label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracies')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucS7X23fJLyH"
   },
   "source": [
    "Now, change your model to achieve at least 75% accuracy on validation set. You can change the model you've implemented, the optimizer, and the augmentations. \n",
    "\n",
    "Looking at the loss and accuracy plots, can you see if your model overfits the trainig set? Why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Yes, it is overfitting. After only a few epochs, the training and validation losses diverge. The training loss and accuracy continue to improve, while the validation loss and accuracy both level off. Raising the weight decay parameter of the optimizer would help to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7nIBeacLE42"
   },
   "source": [
    "### Testing the model\n",
    "\n",
    "Now, use the `test_dataset` to get the final accuracy of your model. Visualize some correctly and incorrectly classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J7MqgSyGtT-"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Calculate and show the test_dataset accuracy of your model.\n",
    "# 2. Visualize some correctly and incorrectly classified examples.\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "total_num_correct = 0\n",
    "with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            _, num_correct = get_loss_and_correct(model, batch, criterion, device)\n",
    "            total_num_correct += num_correct\n",
    "test_accuracy = total_num_correct / len(test_dataset)\n",
    "print(f'Test accuracy: {test_accuracy:.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wrong_examples = (3, 3)\n",
    "fog, axes = plt.subplots(*num_wrong_examples)\n",
    "axes = axes.ravel()\n",
    "axis_index = 0\n",
    "for example in torch.utils.data.DataLoader(test_dataset, batch_size=1):\n",
    "    _, correct = get_loss_and_correct(model, example, criterion, device)\n",
    "    if not correct:\n",
    "        plt.sca(axes[axis_index])\n",
    "        show_normalized_image(example[0][0])\n",
    "        axis_index += 1\n",
    "        if axis_index == len(axes):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fDrpZ28PT0k"
   },
   "source": [
    "### Visualizing filters\n",
    "\n",
    "In this part, we are going to visualize the output of one of the convolutional layers to see what features they focus on.\n",
    "\n",
    "First, let's get some image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAKNWfnuQNLP"
   },
   "outputs": [],
   "source": [
    "image = validation_dataset[10][0]\n",
    "show_normalized_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh4uNI22U9dR"
   },
   "source": [
    "Now, we are going to 'clip' our model at different points to get different intermediate representation. \n",
    "Clip your model at two or three different points and plot the filters output.\n",
    "\n",
    "In order to clip the model, you can use `model.children()` method. For example, to get output only after the first 4 layers, you can do:\n",
    "\n",
    "```\n",
    "clipped = nn.Sequential(\n",
    "    *list(model.children()[:4])\n",
    ")\n",
    "intermediate_output = clipped(input)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtTbfc6vQ2P1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_intermediate_output(result, title):\n",
    "    \"\"\" Plots the intermediate output of shape\n",
    "        N_FILTERS x H x W\n",
    "    \"\"\"\n",
    "    n_filters = result.shape[1]\n",
    "    N = int(math.sqrt(n_filters))\n",
    "    M = (n_filters + N - 1) // N\n",
    "    assert N * M >= n_filters\n",
    "\n",
    "    fig, axs = plt.subplots(N, M, squeeze=False)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            if i*N + j < n_filters:\n",
    "                axs[i][j].imshow(result[0, i*N + j].cpu().detach())\n",
    "                axs[i][j].axis('off')\n",
    "\n",
    "# TODO: \n",
    "# pick a few intermediate representations from your network and plot them using \n",
    "# the provided function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(model.children())\n",
    "for num_children in range(1, 9, 2):\n",
    "    clipped = nn.Sequential(\n",
    "        *children[:num_children]\n",
    "    )\n",
    "    intermediate_output = clipped(image[None])\n",
    "    if intermediate_output.size()[-1] > 1:\n",
    "        plot_intermediate_output(intermediate_output, f'First {num_children} Children')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGLDXme0RCIp"
   },
   "source": [
    "What can you say about those filters? What features are they focusing on?\n",
    "\n",
    "Anwer:\n",
    "\n",
    "One feature the model seems to focus on is the vertical edge formed by the dog's rear right leg. That's lit up by both of the first two convolutional layers. Another feature is the relatively uniform side of the dog's body. The nose also seems to get some attention, but the tail doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_cnn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

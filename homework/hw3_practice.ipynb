{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "* The model does well and learns quickly when training on single characters\n",
    "* It does a decent job with mulitple characters but isn't perfect\n",
    "    * It doesn't always distinguish between double letters (treats \"aa\" as \"a\")\n",
    "    * The letter e also seems to give it trouble. It was prediciting it at a much\n",
    "      later position than I'd expect.\n",
    "\n",
    "Things to try\n",
    "* Normalization\n",
    "* Train on longer strings\n",
    "* Increase weight of BETWEEN a bit so it doesn't get drowned out by double letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJPLI9ftMAaw"
   },
   "source": [
    "# Energy-Based models and structured prediction\n",
    "\n",
    "In this assignment we're going to work with structured prediction. Structured prediction broadly refers to any problem involving predicting structured values, as opposed to plain scalars. Examples of structured outputs include graphs and text.\n",
    "\n",
    "We're going to work with text. The task is to transcribe a word from an image. The difficulty here is that different words have different lengths, so we can't just have fixed number of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KREhDlCMAa7"
   },
   "source": [
    "## Dataset\n",
    "As always, the first thing to do is implementing the dataset. We're going to create a dataset that creates images of random words. We'll also include some augmentations, such as jitter (moving the character horizontally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg1AZlskMF8d"
   },
   "outputs": [],
   "source": [
    "! mkdir fonts\n",
    "! curl --output fonts/font.zip https://www.fontsquirrel.com/fonts/download/Anonymous\n",
    "! unzip -n fonts/font.zip -d fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhX6fPd7MAa7"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image # PIL is a library to process images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "CHAR_WIDTH = 18\n",
    "CHAR_HEIGHT = 32\n",
    "\n",
    "simple_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(), \n",
    "                                ])\n",
    "\n",
    "class SimpleWordsDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, max_length, len=100, jitter=False, noise=False):\n",
    "        self.max_length = max_length\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.len = len\n",
    "        self.jitter = jitter\n",
    "        self.noise = noise\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.len):\n",
    "            text = ''.join([random.choice(string.ascii_lowercase) for i in range(self.max_length)])\n",
    "            img = self.draw_text(text, jitter=self.jitter, noise=self.noise)\n",
    "            yield img, text\n",
    "\n",
    "    def draw_text(self, text, length=None, jitter=False, noise=False):\n",
    "        if length == None:\n",
    "            length = CHAR_WIDTH * len(text)\n",
    "        img = Image.new('L', (length, 32))\n",
    "        fnt = ImageFont.truetype(\"fonts/Anonymous.ttf\", 20)\n",
    "\n",
    "        d = ImageDraw.Draw(img)\n",
    "        pos = (0, 5)\n",
    "        if jitter:\n",
    "            pos = (random.randint(0, 7), 5)\n",
    "        else:\n",
    "            pos = (0, 5)\n",
    "        d.text(pos, text, fill=1, font=fnt)\n",
    "\n",
    "        img = self.transforms(img)\n",
    "        img[img > 0] = 1 \n",
    "\n",
    "        if noise:\n",
    "            img += torch.bernoulli(torch.ones_like(img) * 0.1)\n",
    "            img = img.clamp(0, 1)\n",
    "\n",
    "\n",
    "        return img[0]\n",
    "\n",
    "sds = SimpleWordsDataset(1, jitter=True, noise=False)\n",
    "img = next(iter(sds))[0]\n",
    "print(img.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEc69CZ_MAbK"
   },
   "source": [
    "We can look at what the entire alphabet looks like in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2QnA4u0MAbK"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 9, figsize=(12, 6), dpi=200)\n",
    "\n",
    "max_letter_width = 0\n",
    "for i, c in enumerate(string.ascii_lowercase):\n",
    "    row = i // 9\n",
    "    col = i % 9\n",
    "    letter_img = sds.draw_text(c)\n",
    "    col_maxes = letter_img.max(axis=0).values\n",
    "    assert col_maxes.numel() == CHAR_WIDTH\n",
    "    letter_width = col_maxes.nonzero()[-1] + 1\n",
    "    max_letter_width = max(max_letter_width, letter_width.item())\n",
    "    ax[row][col].imshow(letter_img)\n",
    "    ax[row][col].axis('off')\n",
    "ax[2][8].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG9jGHccMAbL"
   },
   "source": [
    "We can also put the entire alphabet in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BMT8uIFMAbM"
   },
   "outputs": [],
   "source": [
    "alphabet = sds.draw_text(string.ascii_lowercase, 14*26)\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(alphabet)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MreUfYMLMAbM"
   },
   "source": [
    "## Model definition\n",
    "Before we define the model, we define the size of our alphabet. Our alphabet consists of lowercase English letters, and additionally a special character used for space between symbols or before and after the word. For the first part of this assignment, we don't need that extra character.\n",
    "\n",
    "Our end goal is to learn to transcribe words of arbitrary length. However, first, we pre-train our simple convolutional neural net to recognize single characters. In order to be able to use the same model for one character and for entire words, we are going to design the model in a way that makes sure that the output size for one character (or when input image size is 32x18) is 1x27, and Kx27 whenever the input image is wider. K here will depend on particular architecture of the network, and is affected by strides, poolings, among other things. \n",
    "A little bit more formally, our model $f_\\theta$, for an input image $x$ gives output energies $l = f_\\theta(x)$. If $x \\in \\mathbb{R}^{32 \\times 18}$, then $l \\in \\mathbb{R}^{1 \\times 27}$.\n",
    "If $x \\in \\mathbb{R}^{32 \\times 100}$ for example, our model may output $l \\in \\mathbb{R}^{10 \\times 27}$, where $l_i$ corresponds to a particular window in $x$, for example from $x_{0, 9i}$ to $x_{32, 9i + 18}$ (again, this will depend on the particular architecture).\n",
    "\n",
    "Below is a drawing that explains the sliding window concept. We use the same neural net with the same weights to get $l_1, l_2, l_3$, the only difference is receptive field. $l_1$ is looks at the leftmost part, at character 'c', $l_2$ looks at 'a', and $l_3$ looks at 't'. The receptive field may or may not overlap, depending on how you design your convolutions.\n",
    "\n",
    "![cat.png](https://i.imgur.com/JByfyKh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQWNtXbkMAbO"
   },
   "outputs": [],
   "source": [
    "# constants for number of classes in total, and for the special extra character for empty space\n",
    "ALPHABET_SIZE = 27\n",
    "BETWEEN = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzj7g7fXMAbO"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleNet(torch.nn.Module):   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        If single_char is True, the output will have size 1 in width (the last\n",
    "        dimension). Otherwise, the output width will vary depending on the width\n",
    "        of the input.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: try normalization\n",
    "        self.cnn_block = torch.nn.Sequential(\n",
    "            # (batch_size, num_channels, img_height, img_width)\n",
    "            # (B, 1, 32, 18 | 10)\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=10,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            # (B, 10, 32, 18 | 10)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "            ),\n",
    "            # (B, 10, 16, 9 | 5)\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=10,\n",
    "                out_channels=30,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "            ),\n",
    "            # (B, 30, 8, 5 | 3)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(\n",
    "                kernel_size=(8, 1),\n",
    "            ),\n",
    "            # (B, 30, 1, 3 | 1)\n",
    "            # Last later is another convolution rather than a linear layer\n",
    "            # because the width isn't known.\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=30,\n",
    "                out_channels=ALPHABET_SIZE,\n",
    "                kernel_size=1,\n",
    "            )\n",
    "            # (alphabet_size, B, 1, W1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, single_char: bool = False, verbose: bool = False):\n",
    "        if verbose: print(f\"{x.shape=}\")\n",
    "        # Insert new dimension for the number of channels\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        \"\"\"\n",
    "        if not x.ndim == 3:\n",
    "            raise ValueError(\n",
    "                \"Input must have shape \"\n",
    "                \"(batch_size, img_height, img_width). \"\n",
    "                f\"Actual shape: {x.shape}.\"\n",
    "            )\n",
    "        batch_size = x.size(0)\n",
    "        # Insert new dimension for the number of channels\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        # Break original input into sub-images that act like different elements\n",
    "        # of a single batch. This is equivalent to convolving with a kernel of\n",
    "        # all 1s. Would that be more efficient?\n",
    "        # FIXME: Is this valid?\n",
    "        sub_images = tuple(\n",
    "            x[b, :, :, w:w + self.window_width]\n",
    "            for b in range(batch_size)\n",
    "            for w in range(max(x.size(3) - self.window_width + 1, 1))\n",
    "        )\n",
    "        if verbose: print(\n",
    "            f\"{len(sub_images)=}\",\n",
    "            f\"{sub_images[0].shape=}\",\n",
    "            f\"{sub_images[-1].shape=}\"\n",
    "        )\n",
    "        # Make sure each sub-image is the same size by padding with 0s\n",
    "        sub_images = torch.nn.utils.rnn.pad_sequence(\n",
    "            sub_images, batch_first=True\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"{sub_images.shape=}\")\n",
    "            for b in sub_images:\n",
    "                plt.imshow(sub_images[b])\n",
    "        z0 = self.cnn_block(sub_images)\n",
    "        # Resize to be (batch_size, alphabet_size, width)\n",
    "        z1 = torch.stack(z0.split(batch_size, dim=0), dim=0)\n",
    "        if verbose: print(f\"{z1.shape=}\")\n",
    "        return z1.permute(1, 2, 0)\n",
    "        \"\"\"\n",
    "        z0 = self.cnn_block(x)\n",
    "        if verbose: print(f\"{z0.shape=}\")\n",
    "        if single_char:\n",
    "            z0 = nn.functional.adaptive_max_pool2d(z0, (1, 1))\n",
    "            if verbose: print(f\"pooled shape={z0.shape}\")\n",
    "        # z0 has shape (batch_size, alphabet_size, height, width)\n",
    "        # Squeeze out the height since it's not useful\n",
    "        z1 = z0.squeeze(2)\n",
    "        if verbose: print(f\"{z1.shape=}\")\n",
    "        return z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbW54IbwMAbP"
   },
   "source": [
    "Let's initalize the model and apply it to the alphabet image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKTeZ3V1MAbP"
   },
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "with torch.no_grad():\n",
    "    alphabet_energies = model(alphabet.unsqueeze(0), verbose=True)\n",
    "\n",
    "def plot_energies(ce):\n",
    "    fig=plt.figure(dpi=200)\n",
    "    ax = plt.axes()\n",
    "    im = ax.imshow(ce.cpu())\n",
    "    \n",
    "    ax.set_xlabel('window locations →')\n",
    "    ax.set_ylabel('← classes')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(im, cax=cax) \n",
    "    \n",
    "plot_energies(alphabet_energies[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = sds.draw_text(\"a\").unsqueeze(0)\n",
    "_ = model(letter, single_char=False, verbose=True)\n",
    "print()\n",
    "_ = model(letter, single_char=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcVdVdZCMAbQ"
   },
   "source": [
    "So far we only see random outputs, because the classifier is untrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm7ANbUIMAbQ"
   },
   "source": [
    "## Train with one character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN5j048fMAbR"
   },
   "source": [
    "Now we train the model we've created on a dataset where images contain only single characters. Note the changed cross_entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(energies, *args, **kwargs):\n",
    "    \"\"\" We use energies, and therefore we need to use log soft arg min instead\n",
    "        of log soft arg max. To do that we just multiply energies by -1. \"\"\"\n",
    "    return nn.functional.cross_entropy(-1 * energies, *args, **kwargs)\n",
    "\n",
    "def simple_collate_fn(samples):\n",
    "    images, annotations = zip(*samples)\n",
    "    images = list(images)\n",
    "    annotations = list(annotations)\n",
    "    annotations = list(map(lambda c : torch.tensor(ord(c) - ord('a')), annotations))\n",
    "    # This code was provided with the notebook, but this section could be\n",
    "    # simplified by using torch.nn.utils.rnn.pad_sequence().\n",
    "    m_width = max(CHAR_WIDTH, max([i.shape[1] for i in images]))\n",
    "    for i in range(len(images)):\n",
    "        images[i] = torch.nn.functional.pad(images[i], (0, m_width - images[i].shape[-1]))\n",
    "\n",
    "    if len(images) == 1:\n",
    "        return images[0].unsqueeze(0), torch.stack(annotations)\n",
    "    else:\n",
    "        return torch.stack(images), torch.stack(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DccqG9qKMAbR"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_WORKERS = 0\n",
    "DATASET_LENGTH = 10000\n",
    "\n",
    "sds = SimpleWordsDataset(1, len=DATASET_LENGTH, jitter=True, noise=False)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    sds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=simple_collate_fn,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "for i, (images, labels) in enumerate(dataloader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.unsqueeze(dim=1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    energies = model(images, single_char=True)\n",
    "    loss = cross_entropy(energies, labels)\n",
    "    losses.append(loss.item())\n",
    "    predictions = energies.argmin(dim=1)\n",
    "    correct = (predictions == labels).sum()\n",
    "    accuracies.append(correct / BATCH_SIZE)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "axes[0].plot(losses)\n",
    "axes[1].plot(accuracies)\n",
    "axes[0].set_ylabel(\"Training Loss\")\n",
    "axes[1].set_ylabel(\"Training Accuracy\")\n",
    "axes[1].set_xlabel(\"Batch Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataset):\n",
    "    cnt = 0\n",
    "    for i, l in dataset:\n",
    "        with torch.no_grad():\n",
    "            energies = model(i.unsqueeze(0).to(device), single_char=True)\n",
    "        x = energies.argmin(dim=1)[0, 0]\n",
    "        cnt += int(x == (ord(l[0]) - ord('a')))\n",
    "    return cnt / len(dataset)\n",
    "\n",
    "tds = SimpleWordsDataset(1, len=1000)\n",
    "accuracy = get_accuracy(model, tds)\n",
    "print(f\"Accuracy: {accuracy:.3%}\")\n",
    "assert accuracy == 1.0, 'Your model doesn\\'t achieve 100% accuracy for 1 character'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7A8Z2pIMAbT"
   },
   "source": [
    "Now, to see how our model would work with more than one character, we apply the model to a bigger input - the image of the alphabet we saw earlier. We extract the energies for each window and show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k27gXaZgMAbT"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    alphabet_energies_post_train = model(alphabet.to(device).view(1, *alphabet.shape))\n",
    "print(f\"{alphabet_energies_post_train.shape=}\")\n",
    "plot_energies(alphabet_energies_post_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV02yzglMAbT"
   },
   "source": [
    "Explain any classes that are lit up. What is still missing to be able to use it for transcription of words?\n",
    "\n",
    "Answer: The final class (`BETWEEN` == 26, where `\"a\"` through `\"z\"` are 0 through 25) has a high energy across the entire image. That class corresponds to \"no valid character\". None of the training examples included it, so the model learned to always assign it a high energy. We need to train on multicharacter examples which include class 26 so that the model better learns how to divide a string into characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnCgx5k-MAbT"
   },
   "source": [
    "## Training with multiple characters\n",
    "\n",
    "Now, we want to train our model to not only recognize the letters, but also to recognize space in-between so that we can use it for transcription later.\n",
    "\n",
    "This is where complications begin. When transcribing a word from an image, we don't know beforehand how long the word is going to be. We can use our convolutional neural network we've pretrained on single characters to get prediction of character probabilities for all the positions of an input window in the new input image, but we don't know beforehand how to match those predictions with the target label. Training with incorrect matching can lead to wrong model, so in order to be able to train a network to transcribe words, we need a way to find these pairings.\n",
    "\n",
    "![dl.png](https://i.imgur.com/7pnodfV.png)\n",
    "\n",
    "The importance of pairings can be demonstrated by the drawing above. If we map $l_1, l_2, l_3, l_4$ to 'c', 'a', 't', '_' respectively, we'll correctly train the system, but if we put $l_1, l_2, l_3, l_4$ with 'a', 'a', 't', 't', we'd have a very wrong classifier.\n",
    "\n",
    "To formalize this, we use energy-based models' framework. Let's define the energy $E(x, y, z)$ as the sum of cross-entropies for a particular pairing between probabilities our model gives for input image $x$ and text transcription $y$, and pairing $z$. $z$ is a function $z : \\{1, 2, \\dots, \\vert l \\vert \\} \\to \\{1, 2, \\dots, \\vert y \\vert)$, $l$ here is the energies output of our convolutional neural net $l = f_\\theta(x)$. $z$ maps each energy vector in $l$ to an element in the output sequence $y$. We want the mappings to make sense, so $z$ should be a non-decreasing function $z(i) \\leq z(i+1)$, and it shouldn't skip characters, i.e. $\\forall_i \\exists_j z(j)=i$.\n",
    "\n",
    "Energy is then $E(x, y, z) = C(z) + \\sum_{i=1}^{\\vert l \\vert} l_i[z(i)]$\n",
    ",  $C(z)$ is some extra term that allows us to penalize certain pairings, and $l_i[z(i)]$ is the energy of $z(i)$-th symbol on position $i$.\n",
    "\n",
    "In this particular context, we define $C(z)$ to be infinity for impossible pairings:\n",
    "$$C(z) = \\begin{cases}\n",
    "\\infty \\; \\text{if} \\; z(1) \\neq 1 \\vee z(\\vert l \\vert) \\neq \\vert y \\vert \\vee \\exists_{i, 1\\leq 1 \\leq \\vert l \\vert - 1} z(i) > z(i+1) \\vee z(i) < z(i+1) - 1\\\\\n",
    "0 \\; \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, the free energy $F(x, y) = \\arg \\min_z E(x, y, z)$. In other words, the free energy is the energy of the best pairing between the probabilities provided by our model, and the target labels.\n",
    "\n",
    "When training, we are going to use cross-entropies along the best path: $\\ell(x, y, z) = \\sum_{i=1}^{\\vert l \\vert}H(y_{z(i)}, \\sigma(l_i))$, where $H$ is cross-entropy, $\\sigma$ is soft-argmin needed to convert energies to a distribution.\n",
    "\n",
    "First, let's write functions that would calculate the needed cross entropies $H(y_{z(i)}, \\sigma(l_i))$, and energies for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5WJxWjZMAbV"
   },
   "outputs": [],
   "source": [
    "def build_path_matrix(energies, targets):\n",
    "    # inputs: \n",
    "    #    energies, shape is BATCH_SIZE x ALPHABET_SIZE x L\n",
    "    #    targets, shape is BATCH_SIZE x T, elements in range [0, ALPHABET_SIZE)\n",
    "    # L is |l|, i.e. the width of the image in pixels (minus the window size)\n",
    "    # T is |y|, i.e. the number of symbols in the target\n",
    "    # \n",
    "    # outputs:\n",
    "    #    a matrix of shape BATCH_SIZE x L x T\n",
    "    #    where output[i, j, k] = energies[i, targets[i, k], j]\n",
    "    #\n",
    "    # Note: you're not allowed to use for loops. The calculation has to be vectorized.\n",
    "    # you may want to use repeat and repeat_interleave.\n",
    "    B, A, L = energies.shape\n",
    "    # assert A == ALPHABET_SIZE, f\"{energies.size(2)} != {ALPHABET_SIZE}\"\n",
    "    _, T = targets.shape\n",
    "    i, j, k = torch.meshgrid(\n",
    "        torch.arange(B), torch.arange(L), torch.arange(T), indexing=\"ij\"\n",
    "    )\n",
    "    output = energies[i, targets[i, k], j]\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_ce_matrix(energies, targets, weight=None, verbose=False):\n",
    "    # inputs: \n",
    "    #    energies, shape is BATCH_SIZE x ALPHABET_SIZE x L\n",
    "    #    targets, shape is BATCH_SIZE x T\n",
    "    # L is |l|\n",
    "    # T is |y|\n",
    "    # \n",
    "    # outputs:\n",
    "    #    a matrix ce of shape BATCH_SIZE x L x T\n",
    "    #    where ce[i, j, k] = cross_entropy(energies[i, :, j], targets[i, k])\n",
    "    #\n",
    "    # Note: you're not allowed to use for loops. The calculation has to be vectorized.\n",
    "    # you may want to use repeat and repeat_interleave.\n",
    "    B, A, L = energies.shape\n",
    "    # assert A == ALPHABET_SIZE, f\"{energies.size(2)} != {ALPHABET_SIZE}\"\n",
    "    _, T = targets.shape\n",
    "    i, j, k = torch.meshgrid(\n",
    "        torch.arange(B), torch.arange(L), torch.arange(T), indexing=\"ij\"\n",
    "    )\n",
    "    assert i.shape == (B, L, T), f\"{i.shape=} != {(B, L, T)}\"\n",
    "    if verbose: print(f\"{energies.shape=}\")\n",
    "    if verbose: print(f\"{i.shape=}\")\n",
    "    if verbose: print(f\"{energies[i, :, j].shape=}\")\n",
    "    # if verbose: print(f\"{energies=}\")\n",
    "    # if verbose: print(f\"{energies[i, :, j]=}\")\n",
    "    if verbose: print(f\"{targets.shape=}\")\n",
    "    if verbose: print(f\"{targets[i, k].shape=}\")\n",
    "    output = cross_entropy(\n",
    "        energies[i, :, j].permute(0, 3, 1, 2),\n",
    "        targets[i, k],\n",
    "        weight=weight,\n",
    "        reduction=\"none\",\n",
    "    )\n",
    "    # TODO: clean this up\n",
    "    if verbose: print(f\"{output.shape=}\")\n",
    "    output_naive = torch.full_like(output, torch.nan)\n",
    "    for i in range(B):\n",
    "        for j in range(L):\n",
    "            for k in range(T):\n",
    "                output_naive[i, j, k] = cross_entropy(\n",
    "                    energies[i, :, j], targets[i, k], weight=weight, reduction=\"none\",\n",
    "                )\n",
    "    if verbose: print(f\"{output=}\")\n",
    "    if verbose: print(f\"{output_naive=}\")\n",
    "    assert torch.allclose(output, output_naive, atol=1e-6), (\n",
    "        f\"{output=} != {output_naive=}\"\n",
    "    )\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for build_path_matrix() and build_ce_matrix()\n",
    "Bt = 2\n",
    "Lt = 4\n",
    "Tt = 5\n",
    "At = 3\n",
    "test_energies = torch.randn((Bt, At, Lt), requires_grad=False)\n",
    "test_targets = torch.tensor([\n",
    "    [0, 1, 2, 1, 0],\n",
    "    [2, 1, 0, 1, 2],\n",
    "])\n",
    "test_pm = build_path_matrix(test_energies, test_targets)\n",
    "assert test_pm.shape == (Bt, Lt, Tt), f\"{test_pm.shape=} != {(Bt, Lt, Tt)}\"\n",
    "for b in range(test_targets.size(0)):\n",
    "    for t in range(test_targets.size(1)):\n",
    "        assert torch.allclose(\n",
    "            test_pm[b, :, t], test_energies[b, test_targets[b,t], :]\n",
    "        ), f\"{test_pm[b, :, t]=} != {test_energies[b, test_targets[b,t], :]=}\"\n",
    "\n",
    "test_energies = torch.tensor([[\n",
    "    [0.0, 100.0, 100.0, 100.0, 100.0],\n",
    "    [100.0, 100.0, 0.0, 100.0, 100.0],\n",
    "    [100.0, 0.0, 100.0, 0.0, 0.0],\n",
    "]])\n",
    "print(f\"{test_energies.shape=}\")\n",
    "test_targets = torch.tensor([[0, 2, 1, 2]])\n",
    "test_cem = build_ce_matrix(test_energies, test_targets, verbose=True)\n",
    "print(f\"{test_cem.shape=}\\n{test_cem=}\")\n",
    "for b in range(test_cem.size(0)):\n",
    "    for l in range(test_cem.size(1)):\n",
    "        for t in range(test_cem.size(2)):\n",
    "            expected_loss = cross_entropy(\n",
    "                test_energies[b, :, l],\n",
    "                test_targets[b, t],\n",
    "            )\n",
    "            assert torch.allclose(\n",
    "                test_cem[b, l, t], expected_loss, atol=1e-6,\n",
    "            ), f\"{test_cem[b, l, t]=} != {expected_loss=}\"\n",
    "\n",
    "test_pm = build_path_matrix(test_energies, test_targets)[0]\n",
    "test_path_energy, test_path, _ = find_path(test_pm)\n",
    "test_path_tensor = torch.tensor(test_path)\n",
    "print(f\"{test_path_tensor.shape=}\\n{test_path_tensor=}\")\n",
    "print(f\"{test_path_energy=}\")\n",
    "plot_pm(test_pm.T, test_path)\n",
    "test_path_loss = test_cem[0, test_path_tensor[:,0], test_path_tensor[:,1]].sum()\n",
    "print(f\"{test_path_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, T = 16, 300, 25\n",
    "dummy_energies = torch.ones((B, ALPHABET_SIZE, L))\n",
    "dummy_targets = torch.ones((B, T), dtype=torch.int64)\n",
    "print(f\"{dummy_energies.shape=}, {dummy_targets.shape=}\")\n",
    "o1 = build_path_matrix(dummy_energies, dummy_targets)\n",
    "print(f\"{o1.shape=}\")\n",
    "assert o1.shape == (B, L, T), f\"{o1.shape=} != {(B, L, T)}\"\n",
    "\n",
    "o2 = build_ce_matrix(dummy_energies, dummy_targets)\n",
    "print(f\"{o2.shape=}\")\n",
    "assert o2.shape == (B, L, T), F\"{o2.shape=} != {(B, L, T)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKZLjSCFMAbV"
   },
   "source": [
    "Another thing we will need is a transformation for our label $y$. We don't want to use it as is, we want to insert some special label after each character, so, for example `cat` becomes `c_a_t_`. This extra `_` models the separation between words, allowing our model to distinguish between strings `aa` and `a` in its output. This is then used in inference - we can just get the most likely character for each position from $l = f_\\theta(x)$ (for example `aa_bb_ccc_`), and then remove duplicate characters (`a_b_c_`), and then remove `_` (`abc`). \n",
    "Let's implement a function that would change the string in this manner, and then map all characters to values from 0 to 26, with 0 to 25 corresponding to a-z, and 26 corresponding to _:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE4oZRDAMAbV"
   },
   "outputs": [],
   "source": [
    "def char_to_num(char: str) -> int:\n",
    "    if char == \"_\":\n",
    "        return 26\n",
    "    return ord(char) - ord(\"a\")\n",
    "\n",
    "\n",
    "def num_to_char(num: int) -> str:\n",
    "    char = chr(num + ord(\"a\"))\n",
    "    if char <= \"z\":\n",
    "        return char\n",
    "    return \"_\"\n",
    "\n",
    "\n",
    "def transform_word(s):\n",
    "    # input: a string\n",
    "    # output: a tensor of shape 2*len(s)\n",
    "    underscored = \"_\".join(s) + \"_\"\n",
    "    return torch.tensor(\n",
    "        [char_to_num(char) for char in underscored], dtype=torch.int64\n",
    "    )\n",
    "\n",
    "    \n",
    "# Unit tests\n",
    "assert char_to_num(\"a\") == 0, f\"{char_to_num('a')} != 0\"\n",
    "assert char_to_num(\"b\") == 1, f\"{char_to_num('b')} != 1\"\n",
    "assert char_to_num(\"z\") == 25, f\"{char_to_num('z')} != 25\"\n",
    "assert char_to_num(\"_\") == 26, f\"{char_to_num('_')} != 26\"\n",
    "assert num_to_char(0) == \"a\", f\"{num_to_char(0)} != 'a'\"\n",
    "assert num_to_char(1) == \"b\", f\"{num_to_char(1)} != 'b'\"\n",
    "assert num_to_char(25) == \"z\", f\"{num_to_char(25)} != 'z'\"\n",
    "assert num_to_char(26) == \"_\", f\"{num_to_char(26)} != '_'\"\n",
    "assert (transform_word(\"\") == torch.tensor([])).all()\n",
    "assert (transform_word(\"a\") == torch.tensor([0, 26])).all()\n",
    "assert (transform_word(\"abc\") == torch.tensor([0, 26, 1, 26, 2, 26])).all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "i-txyVZkMAbV"
   },
   "source": [
    "Now, let's plot energy table built on our model's prediction for alphabet image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgJsmkzmMAbX"
   },
   "outputs": [],
   "source": [
    "def plot_pm(energies, path=None):\n",
    "    fig=plt.figure(dpi=200)\n",
    "    ax = plt.axes()\n",
    "    im = ax.imshow(energies.to(device))\n",
    "    \n",
    "    ax.set_xlabel('window locations →')\n",
    "    ax.set_ylabel('← label characters')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    if path is not None:\n",
    "        for i in range(len(path) - 1):\n",
    "            ax.plot(*path[i], *path[i+1], marker = 'o', markersize=0.5, linewidth=10, color='r', alpha=1)\n",
    "\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(im, cax=cax) \n",
    "\n",
    "with torch.no_grad():\n",
    "    energies = model(alphabet.to(device).unsqueeze(dim=0))\n",
    "targets = transform_word(string.ascii_lowercase).unsqueeze(0)\n",
    "\n",
    "pm = build_path_matrix(energies, targets)\n",
    "plot_pm(energies[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_0IlGAzMAba"
   },
   "source": [
    "What do you see? What does the model classify correctly, and what does it have problems with?\n",
    "\n",
    "Answer: The heatmap is identical to the plot above since we reran the model on the same input (the full alphabet image) without doing any retraining in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LvDnVQLMAba"
   },
   "source": [
    "Searching for a good pairing $z$ is same as searching for a trajectory with a small sum of it's values in this `pm` matrix. Where does the trajectory start, and where does it end? What other properties does the trajectory have? Can you see where an optimal trajecotry would be passing through in the plot above?\n",
    "\n",
    "Answer: The ideal path for this input image (the full alphabet) would start in the top-left at a character label of `a` (0) for window location 0. It would generally trend down and to the left, ending up at a label of `z` (25) for the largest window location. Between characters, the path would drop down to a character label of `_` (26) to represent the fact that the window isn't aligned with a character at those locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3DOa35NMAba"
   },
   "source": [
    "Now let's implement a function that would tell us the energy of a particular path (i.e. pairing).\n",
    "\n",
    "Energy is then $E(x, y, z) = C(z) + \\sum_{i=1}^{\\vert l \\vert} l_i[z(i)]$\n",
    ",  $C(z)$ is some extra term that allows us to penalize certain pairings, and $l_i[z(i)]$ is the energy of $z(i)$-th symbol on position $i$.\n",
    "\n",
    "In this particular context, we define $C(z)$ to be infinity for impossible pairings:\n",
    "$$C(z) = \\begin{cases}\n",
    "\\infty \\; \\text{if} \\; z(1) \\neq 1 \\vee z(\\vert l \\vert) \\neq \\vert y \\vert \\vee \\exists_{i, 1\\leq 1 \\leq \\vert l \\vert - 1} z(i) > z(i+1) \\vee z(i) < z(i+1) - 1\\\\\n",
    "0 \\; \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wxaq_x9MAbb"
   },
   "outputs": [],
   "source": [
    "def path_energy(pm, path, verbose=False):\n",
    "    # inputs:\n",
    "    #   pm - a matrix of energies \n",
    "    #    L - energies length\n",
    "    #    T - targets length\n",
    "    #   path - list of length L that maps each energy vector to an element in T\n",
    "    # returns:\n",
    "    #   energy - sum of energies on the path, or 2**30 if the mapping is invalid\n",
    "    L, T = pm.shape\n",
    "    assert len(path) == L, f\"{len(path)} != {L}\"\n",
    "    INVALID = torch.tensor(2**30)\n",
    "    if path[0] != 0:\n",
    "        if verbose: print(f\"Starts at {path[0]=} != 0\")\n",
    "        return INVALID\n",
    "    if path[-1] != T - 1:\n",
    "        if verbose: print(f\"Ends at {path[-1]=} != {T - 1}\")\n",
    "        return INVALID\n",
    "    if any(path[i] > path[i+1] for i in range(L - 1)):\n",
    "        if verbose: print(f\"Decreases\")\n",
    "        return INVALID\n",
    "    if any(path[i] < path[i+1] - 1 for i in range(L - 1)):\n",
    "        if verbose: print(f\"Skips\")\n",
    "        return INVALID\n",
    "    if verbose: print(\"Valid path\")\n",
    "    w = pm[torch.arange(L), path]\n",
    "    return w.sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "b-xYfH4OMAbb"
   },
   "source": [
    "Now we can check some randomly generated paths and see the associated energies for our alphabet image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnBl2XQGMAbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = torch.zeros(energies.shape[2] - 1)\n",
    "path[:targets.shape[1] - 1] = 1\n",
    "path = [0] + list(map(lambda x : x.int().item(), path[torch.randperm(path.shape[0])].cumsum(dim=-1)))\n",
    "points = list(zip(range(energies.shape[2]), path))\n",
    "\n",
    "# TODO: why is this striped, alternating between high and low values every row?\n",
    "# It's because the BETWEEN character comes after each letter and has a very high\n",
    "# energy.\n",
    "plot_pm(pm[0].T.detach(), points)\n",
    "energy = path_energy(pm[0], path, verbose=True)\n",
    "print(f\"energy is {energy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJrzwsTVTbl1"
   },
   "source": [
    "Now, generate two paths with the worst possible energy, print their energies and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93Km7zUQTlxe"
   },
   "outputs": [],
   "source": [
    "# The worse possible energy is one for an invalid path,\n",
    "# such as one that's not monotionc.\n",
    "t = torch.arange(energies.size(2))\n",
    "jump_path = torch.zeros_like(t)\n",
    "jump_path[energies.size(2) // 2:] = targets.size(1) - 1\n",
    "jump_points = list(zip(t, jump_path))\n",
    "plot_pm(pm[0].T.detach(), jump_points)\n",
    "jump_energy = path_energy(pm[0], jump_path, verbose=True).item()\n",
    "print(f\"Jump path energy: {jump_energy}\")\n",
    "\n",
    "sin_path = targets.size(1) * (torch.sin(2 * torch.pi * t / energies.size(2)) / 2 + 0.5)\n",
    "sin_points = list(zip(t, sin_path))\n",
    "plot_pm(pm[0].T.detach(), sin_points)\n",
    "sin_energy = path_energy(pm[0], sin_path, verbose=True).item()\n",
    "print(f\"Sine wave energy: {sin_energy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnrQtZ-kMAbd"
   },
   "source": [
    "### Optimal path finding\n",
    "Now, we're going to implement the finding of the optimal path. To do that, we're going to use Viterbi algorithm, which in this case is a simple dynamic programming problem.\n",
    "In this context, it's a simple dynamic programming algorithm that for each pair i, j, calculates the minimum cost of the path that goes from 0-th index in the energies and 0-th index in the target, to i-th index in the energies, and j-th index in the target. We can memorize the values in a 2-dimensional array, let's call it `dp`. Then we have the following transitions:\n",
    "```\n",
    "dp[0, 0] = pm[0, 0]\n",
    "dp[i, j] = min(dp[i - 1, j], dp[i - 1, j - 1]) + pm[i, j]\n",
    "```\n",
    "\n",
    "The optimal path can be recovered if we memorize which cell we came from for each `dp[i, j]`.\n",
    "\n",
    "Below, you'll need to implement this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyyipYDqMAbd"
   },
   "outputs": [],
   "source": [
    "# This cell doesn't use the path_energy() function\n",
    "# even though it seems like it should.\n",
    "\n",
    "def energy_to_point(pm, i, j, dp):\n",
    "    if (i, j) in dp:\n",
    "        return dp[(i, j)]\n",
    "    if i < j:\n",
    "        raise Exception(f\"Invalid path destination: ({i=}, {j=})\")\n",
    "    if i == 0 and j == 0:\n",
    "        energy = 0\n",
    "        path = ()\n",
    "    elif j == 0:\n",
    "        energy, path = energy_to_point(pm, i - 1, j, dp)\n",
    "    elif i == j:\n",
    "        energy, path = energy_to_point(pm, i - 1, j - 1, dp)\n",
    "    else:\n",
    "        assert i != 0 and j != 0\n",
    "        assert i > j\n",
    "        energy, path = min(\n",
    "            energy_to_point(pm, i - 1, j, dp),\n",
    "            energy_to_point(pm, i - 1, j - 1, dp),\n",
    "        )\n",
    "    energy = energy + pm[i, j]\n",
    "    path = path + ((i, j),)\n",
    "    path_js = [p[1] for p in path]\n",
    "    valid_energy = path_energy(pm[:i + 1, :j + 1], path_js)\n",
    "    if not torch.isclose(energy, valid_energy, atol=1e-3):\n",
    "        print(f\"Found path to ({i}, {j}) is invalid\")\n",
    "        print(\"Naive energy:\", energy)\n",
    "        print(\"Validated energy\", valid_energy)\n",
    "        print(path_js)\n",
    "        raise Exception(\"Energy discrepancy\")\n",
    "    dp[i, j] = (energy, path)\n",
    "    return energy, path\n",
    "\n",
    "\n",
    "def find_path(pm):\n",
    "    # inputs:\n",
    "    #   pm - a tensor of shape LxT with energies\n",
    "    #     L is length of energies array\n",
    "    #     T is target sequence length\n",
    "    # NOTE: this is slow because it's not vectorized to work with batches.\n",
    "    #  output:\n",
    "    #     a tuple of three elements:\n",
    "    #         1. sum of energies on the best path,\n",
    "    #         2. list of tuples - points of the best path in the pm matrix \n",
    "    #         3. the dp array\n",
    "    dp_dict = {}\n",
    "    min_energy, best_path = energy_to_point(\n",
    "        pm, pm.size(0) - 1, pm.size(1) - 1, dp_dict\n",
    "    )\n",
    "    dp_array = torch.full_like(pm, torch.nan)\n",
    "    for (i, j) in dp_dict:\n",
    "        dp_array[i, j] = dp_dict[(i, j)][0]\n",
    "    return min_energy, best_path, dp_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svhNOp8XMAbd"
   },
   "source": [
    "Let's take a look at the best path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LcKBvPBMAbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "free_energy, path, d = find_path(pm[0])\n",
    "plot_pm(pm[0].T.detach(), path)\n",
    "print('free energy is', free_energy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBlQsXB0oLpu"
   },
   "source": [
    "We can also visualize the dp array. You may need to tune clamping to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g2ZAP5p92zt"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(d.cpu().detach().T)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmIhBQ5vMAbe"
   },
   "source": [
    "### Training loop\n",
    "Now is time to train the network using our best path finder. We're going to use the energy loss function:\n",
    "$$\\ell(x, y) = \\sum_i H(y_{z(i)}, l_i)$$\n",
    "Where $z$ is the best path we've found. This is akin to pushing down on the free energy $F(x, y)$, while pushing up everywhere else by nature of cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2HLpKuVMAbe"
   },
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    \"\"\" A function to collate samples into batches for multi-character case\"\"\"\n",
    "    images, annotations = zip(*samples)\n",
    "    images = list(images)\n",
    "    annotations = list(annotations)\n",
    "    annotations = list(map(transform_word, annotations))\n",
    "    m_width = max(CHAR_WIDTH, max([i.shape[1] for i in images]))\n",
    "    m_length = max(3, max([s.shape[0] for s in annotations]))\n",
    "    for i in range(len(images)):\n",
    "        images[i] = torch.nn.functional.pad(images[i], (0, m_width - images[i].shape[-1]))\n",
    "        annotations[i] = torch.nn.functional.pad(annotations[i], (0, m_length - annotations[i].shape[0]), value=BETWEEN)\n",
    "    if len(images) == 1:\n",
    "        return images[0].unsqueeze(0), torch.stack(annotations)\n",
    "    else:\n",
    "        return torch.stack(images), torch.stack(annotations)\n",
    "\n",
    "WORD_LENGTH = 2\n",
    "sds = SimpleWordsDataset(WORD_LENGTH, 30_000)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    sds, batch_size=BATCH_SIZE, num_workers=0, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# TODO: train the model\n",
    "# note: remember that our best path finding algorithm is not batched, so you'll\n",
    "# need a for loop to do loss calculation. \n",
    "# This is not ideal, as for loops are very slow, but for \n",
    "# demonstration purposes it will suffice. In practice, this will be\n",
    "# unusable for any real problem unless it handles batching.\n",
    "\n",
    "# also: remember that the loss is the sum of cross_entropies along the path, not \n",
    "# energies!\n",
    "\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "losses = []\n",
    "# Down-weight the BETWEEN character since it appears so often\n",
    "loss_weights = torch.ones(ALPHABET_SIZE)\n",
    "loss_weights[BETWEEN] = 1 / ALPHABET_SIZE\n",
    "\n",
    "# model.register_full_backward_hook(\n",
    "#     lambda model, grad_input, grad_output: print(grad_input, grad_output, \"\", sep=\"\\n\")\n",
    "# )\n",
    "\n",
    "for i, (images, targets) in enumerate(dataloader):\n",
    "    assert images.shape == (\n",
    "        BATCH_SIZE, CHAR_HEIGHT, CHAR_WIDTH * WORD_LENGTH\n",
    "    ), (\n",
    "        f\"{images.shape=} != {(BATCH_SIZE, CHAR_HEIGHT, CHAR_WIDTH * WORD_LENGTH)=}\"\n",
    "    )\n",
    "    assert (\n",
    "        targets.shape == (BATCH_SIZE, 2 * WORD_LENGTH)\n",
    "    ), (\n",
    "        f\"{targets.shape=} != {(BATCH_SIZE, 2 * WORD_LENGTH)=}\"\n",
    "    )\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    energies = model(images)\n",
    "    assert energies.ndim == 3 and energies.shape[:2] == (\n",
    "        BATCH_SIZE, ALPHABET_SIZE\n",
    "    ), f\"{energies.shape=} != ({BATCH_SIZE=}, {ALPHABET_SIZE=}, W)\"\n",
    "    path_matrix = build_path_matrix(energies, targets)\n",
    "    cross_entropy_matrix = build_ce_matrix(energies, targets, loss_weights)\n",
    "    assert path_matrix.shape == cross_entropy_matrix.shape\n",
    "    assert path_matrix.shape == (\n",
    "        BATCH_SIZE, energies.size(2), 2 * WORD_LENGTH\n",
    "    ), (\n",
    "        f\"{path_matrix.shape=} != \"\n",
    "        f\"{(BATCH_SIZE, energies.size(2), 2 * WORD_LENGTH)=}\"\n",
    "    )\n",
    "    loss = torch.tensor(0.0, requires_grad=True)\n",
    "    for b in range(path_matrix.size(0)):\n",
    "        _, path, _ = find_path(path_matrix[b])\n",
    "        path_tensor = torch.tensor(path)\n",
    "        cem = cross_entropy_matrix[b]\n",
    "        assert (\n",
    "            cem.shape == (energies.size(2), 2 * WORD_LENGTH)\n",
    "        ), (\n",
    "            f\"{cem.shape=} != {(energies.size(2), 2 * WORD_LENGTH)=}\"\n",
    "        )\n",
    "        path_loss = cem[path_tensor[:,0], path_tensor[:,1]]\n",
    "        loss = loss + path_loss.sum()\n",
    "    loss = loss / BATCH_SIZE\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Batch iteration: {i}, loss: {loss:.3f}     \", end=\"\\r\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Batch iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pm(path_matrix[-1].detach().T, path)\n",
    "print(images.shape)\n",
    "print(path_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy, path2, heatmap = find_path(path_matrix[-1])\n",
    "plt.imshow(heatmap.detach().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_image_full, letter_label = next(iter(SimpleWordsDataset(2, 1)))\n",
    "letter_image = letter_image_full[:, :]\n",
    "plt.imshow(letter_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    letter_energies = model(letter_image.unsqueeze(0)).squeeze(0).squeeze(-1)\n",
    "print(f\"{letter_energies.shape=}\")\n",
    "# print(f\"{letter_energies=}\")\n",
    "# sorted_indices = torch.argsort(letter_energies[:,0])\n",
    "# for ind in sorted_indices:\n",
    "#     print(num_to_char(ind), letter_energies[ind,0])\n",
    "min_energy_indices = torch.argmin(letter_energies, 0)\n",
    "for x in range(letter_energies.size(1)):\n",
    "    print(\n",
    "        x,\n",
    "        num_to_char(min_energy_indices[x].item()),\n",
    "        letter_energies[min_energy_indices[x], x],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image, example_label = next(iter(SimpleWordsDataset(WORD_LENGTH, 1)))\n",
    "print(f\"{example_image.shape=}, {example_label=}\")\n",
    "plt.imshow(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_target = transform_word(example_label)\n",
    "print(f\"{example_target=}\")\n",
    "with torch.no_grad():\n",
    "    example_energies = model(example_image.unsqueeze(0))\n",
    "print(f\"{example_energies.shape=}\")\n",
    "plt.imshow(example_energies[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pm = build_path_matrix(example_energies, example_target.unsqueeze(0))\n",
    "print(f\"{example_pm.shape=}\")\n",
    "example_min_energy, example_path, example_dp = find_path(example_pm[0])\n",
    "print(f\"{example_min_energy=}, {example_path=}\")\n",
    "plot_pm(example_pm[0].detach().T, example_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8N_ACMfMAbg"
   },
   "source": [
    "Let's check what the energy matrix looks like for the alphabet image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf7yuVcWMAbg"
   },
   "outputs": [],
   "source": [
    "energies = model(alphabet.unsqueeze(0).to(device))\n",
    "targets = transform_word(string.ascii_lowercase)\n",
    "pm = build_path_matrix(energies, targets.unsqueeze(0))\n",
    "\n",
    "free_energy, path, _ = find_path(pm[0])\n",
    "plot_pm(pm[0].detach().T, path)\n",
    "print('free energy is', free_energy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUoXSmvQMAbg"
   },
   "source": [
    "Explain how the free energy changed, and why.\n",
    "\n",
    "Answer: #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QcTNZp6MAbg"
   },
   "source": [
    "We can also look at raw energies output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgUyDNBSMAbg"
   },
   "outputs": [],
   "source": [
    "alphabet_energy_post_train_viterbi = model(alphabet.to(device).view(1, *alphabet.shape))\n",
    "\n",
    "plt.figure(dpi=200, figsize=(40, 10))\n",
    "plt.imshow(alphabet_energy_post_train_viterbi.cpu().data[0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWDtL_AoMAbh"
   },
   "source": [
    "How does this compare to the energies we had after training only on one-character dataset?\n",
    "\n",
    "Answer: #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x7kaFxTMAbh"
   },
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w8ZojnnMAbh"
   },
   "source": [
    "Now we can use the model for decoding a word from an image. Let's pick some word, apply the model to it, and see energies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaTiwIthMAbh"
   },
   "outputs": [],
   "source": [
    "img = sds.draw_text('hello')\n",
    "energies = model(img.to(device).unsqueeze(0))\n",
    "plt.imshow(img)\n",
    "plot_energies(energies[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxTbUNkYMAbh"
   },
   "source": [
    "You should see some characters light up. Now, let's implement a simple decoding algorithm. To decode, first we want to get most likely classes for all energies, and then do two things:\n",
    "1. segment strings using the divisors (our special character with index 26), and for each segment replace it with the most common character in that segment. Example: aaab_bab_ -> a_b. If some characters are equally common, you can pick random.\n",
    "2. remove all special divisor characters: a_b -> ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "error",
     "timestamp": 1615774686133,
     "user": {
      "displayName": "Uladzislau Sobal",
      "photoUrl": "",
      "userId": "15584353483948327076"
     },
     "user_tz": -60
    },
    "id": "iiamf7wzMAbi",
    "outputId": "923123ed-99b2-419d-f2ac-5778d9370ffa"
   },
   "outputs": [],
   "source": [
    "def indices_to_str(indices):\n",
    "    # inputs: indices - a tensor of most likely class indices\n",
    "    # outputs: decoded string\n",
    "    chunk_start = 0\n",
    "    characters = []\n",
    "    for i, ind in enumerate(indices):\n",
    "        if ind == BETWEEN:\n",
    "            if chunk_start < i:\n",
    "                most_common = indices[chunk_start:i].mode().values.item()\n",
    "                characters.append(num_to_char(most_common))\n",
    "            chunk_start = i + 1\n",
    "    if len(indices) > 0 and ind != BETWEEN:\n",
    "        characters.append(num_to_char(ind))\n",
    "    return \"\".join(characters)\n",
    "\n",
    "# Unit tests for indices_to_str()\n",
    "test_input = torch.tensor([])\n",
    "test_output = \"\"\n",
    "actual_output = indices_to_str(test_input)\n",
    "assert actual_output ==  test_output, (\n",
    "    f\"{actual_output!r} != {test_output!r}\"\n",
    ")\n",
    "test_input = torch.tensor([0])\n",
    "test_output = num_to_char(0)\n",
    "actual_output = indices_to_str(test_input)\n",
    "assert actual_output ==  test_output, (\n",
    "    f\"{actual_output!r} != {test_output!r}\"\n",
    ")\n",
    "test_input = torch.tensor([BETWEEN])\n",
    "test_output = \"\"\n",
    "actual_output = indices_to_str(test_input)\n",
    "assert actual_output ==  test_output, (\n",
    "    f\"{actual_output!r} != {test_output!r}\"\n",
    ")\n",
    "test_input = torch.tensor([0, BETWEEN, BETWEEN])\n",
    "test_output = num_to_char(0)\n",
    "actual_output = indices_to_str(test_input)\n",
    "assert actual_output ==  test_output, (\n",
    "    f\"{actual_output!r} != {test_output!r}\"\n",
    ")\n",
    "test_input = torch.tensor([0, BETWEEN, BETWEEN, 1])\n",
    "test_output = num_to_char(0) + num_to_char(1)\n",
    "actual_output = indices_to_str(test_input)\n",
    "assert actual_output ==  test_output, (\n",
    "    f\"{actual_output!r} != {test_output!r}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_indices = energies[0].argmin(dim=0)\n",
    "print(min_indices)\n",
    "print(indices_to_str(min_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
